<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Syn2Real: A New Benchmark for Synthetic-to-Real Visual Domain Adaptation</title>

    <!-- Bootstrap -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">
    <link href="../css/template.css" rel="stylesheet">
</head>
<body>
<nav class="navbar navbar-inverse navbar-fixed-top" role="navigation"
     style="background-color:#34a853;border-color:#34a853">

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse navbar-ex1-collapse">
        <ul class="nav navbar-nav">
            <li role="presentation"><a href="#overview">Overview</a></li>
            <li role="presentation"><a href="#dataset">Dataset</a></li>
            <li role="presentation"><a href="#code">Code</a></li>
            <li role="presentation"><a href="#refs">Reference</a></li>
            <!--<li><a href="./index.html#Contact">Contact</a></li>-->
        </ul>
        <!-- </div>--> <!--/.navbar-collapse -->

    </div>
</nav>


<div class="container">

    <div class="home-intro" style="padding: 5% 10%">
        <div class="row">
            <h1 align="center">Moment Matching for <br>Multi-Source Domain Adaptation</h1>
        </div>

        <div class="project-page">
            <a name="abstract"></a>
            <h2>Abstract</h2>
            <p class="text-justify">
                Conventional unsupervised domain adaptation (UDA) assumes that training data are sampled from a single
                domain. This neglects the more practical scenario where
                training data are collected from multiple sources, requiring multi-source domain adaptation. We make
                three major contributions towards addressing this problem. First, we
                propose a new deep learning approach, Moment Matching for Multi-Source Domain Adaptation (M3SDA), which
                aims to transfer knowledge learned from multiple labeled source
                domains to an unlabeled target domain by dynamically aligning moments of their feature distributions.
                Second, we provide a sound theoretical analysis of moment-related error
                bounds for multi-source domain adaptation. Third, we collect and annotate by far the largest UDA dataset
                with six distinct domains and approximately 0.6 million images distributed
                among 345 categories, addressing the gap in data availability for multi-source UDA research. Extensive
                experiments are performed to demonstrate the effectiveness of
                our proposed model, which outperforms existing state-ofthe-art methods by a large margin.

            </p>
            <center>
                <a
                        href="http://cs-people.bu.edu/xpeng/pdfs/M3SDA.pdf" target="_blank" class="btn btn-danger"
                        role="button">PDF</a>
            </center>
        </div>

        <hr class="soften"></hr>


        <div class="project-page">
            <a name="overview"></a>
            <h2>Overview</h2>
            <br>
            <center>
                <img width="90%" alt="License" src="imgs/overview.png"></img>
            </center>
            <br>
        </div>

        <div class="project-page">
            <a name="dataset"></a>
            <h2>Dataset</h2>
            <br>
            <center>
                <img width="90%" alt="License" src="imgs/data_examples.png"></img>
                <img width="90%" alt="License" src="imgs/statistics.png"></img>

            </center>
            <br>
        </div>

        <div class="project-page">
            <a name="download"></a>
            <h2>Download</h2>
            <br>


            <br>
        </div>


        <hr class="soften"></hr>
        <div class="project-page">
            <a name="code"></a>
            <h2>Code</h2>
            <p class="lead">

                </br>
                <!--<a href="https://github.com/VisionLearningGroup/visda-2018-public" target="_blank">[openset and-->
                    <!--detection]</a>-->
                <!--<a href="https://github.com/VisionLearningGroup/taskcv-2017-public" target="_blank">[classification and-->
                    <!--segmentation]</a>-->
                TBD
                </br>
            </p>
        </div>


        <hr class="soften"></hr>

        <div class="project-page">

            <a name="refs"></a>
            <h2>Reference</h2>
            <p class="lead">
                If you find this useful in your work please consider citing:
            <div class="highlight">
          <pre> <code>
@article{peng2018moment,
        title={Moment Matching for Multi-Source Domain Adaptation},
        author={Peng, Xingchao and Bai, Qinxun and Xia, Xide and Huang, Zijun and Saenko, Kate and Wang, Bo},
        journal={arXiv preprint arXiv:1812.01754},
        year={2018}
        }
            </code> </pre>


            </div>

        </div>

        <div class="project-page">

            <h2>Fair Use Notice</h2>

<p>This dataset contains some copyrighted material whose use has not been specifically authorized by the copyright owners. In an effort to advance scientific research, we make this material available for academic research. We believe this constitutes a <strong>fair use</strong> of any such copyrighted material as provided for in section 107 of the US Copyright Law. In accordance with Title 17 U.S.C. Section 107, the material on this site is distributed without profit for non-commercial research and educational purposes. For more information on fair use please <a href="https://www.law.cornell.edu/uscode/text/17/107" title="Fair Use Notice">click here</a>. If you wish to use copyrighted material on this site or in our dataset for purposes of your own that go beyond non-commercial research and academic purposes, you must obtain permission directly from the copyright owner.
<em>(adapted from <a href="http://people.cs.pitt.edu/~chris/photographer/">Christopher Thomas</a>)</em></p>


        </div>
    </div> <!--container-->

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <!--<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>-->
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <!--<script src="js/bootstrap.min.js"></script>
    <script src="js/toggle.js"></script> -->
</body>
</html>

