<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Caption Guided Saliency</title>

    <!-- Bootstrap -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">
    <link href="../css/template.css" rel="stylesheet">
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    
      ga('create', 'UA-59771467-2', 'auto');
      ga('send', 'pageview');
    
    </script>
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
          <!-- Brand and toggle get grouped for better mobile display -->
        <div class="container">

          <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
          </div>
    
          <!-- Collect the nav links, forms, and other content for toggling -->
          <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
              <!--<li><a href="./index.html#">Home</a></li>
              <li><a href="./index.html#Research">Research</a></li>
              <li><a href="./index.html#Contact">Contact</a></li>-->
            </ul
          </div><!-- /.navbar-collapse -->

        </div>
    </nav>


    <div class="container">

    <div class="home-intro">
      <div class="row">
          <h1 align="center">Top-down Visual Saliency Guided by Captions</h1>
      </div>
          <div class="highlight-box">
           <h2>
           <div class="center-pills">
           <ul class="nav nav-pills">
          <li role="presentation"> <a href="#abstract">Abstract</a></li> 
          <li role="presentation"> <a href="#overview">Overview</a></li> 
          <li role="presentation"> <a href="#examples">Examples</a></li>
          <li role="presentation"> <a href="#code">Code</a></li> 
          <li role="presentation"> <a href="#refs">Reference</a></li>
           </ul>
          </div>
          </h2>
          </div>

          <div class="project-page">
          <a name="abstract"></a>
          <h2>Abstract</h2>
          <p class="text-justify">
    Neural image/video captioning models can generate accurate descriptions, but their internal process of mapping regions to words is a black box and therefore difficult to explain. 
Top-down neural saliency methods can find important regions given a high-level semantic task such as object classification, but cannot use a natural language sentence as the top-down input for the task. 
In this paper, we propose Caption-Guided Visual Saliency to expose the region-to-word mapping in modern encoder-decoder networks and demonstrate that it is learned implicitly from caption training data, without any pixel-level annotations. Our approach can produce spatial or spatio-temporal heatmaps for both predicted captions, and for arbitrary query sentences.
It recovers saliency without the overhead of introducing explicit attention layers, and can be used to analyze a variety of existing model architectures and improve their design. 
Evaluation on large-scale video and image datasets demonstrates that our
approach achieves comparable captioning performance with existing methods while providing more accurate saliency heatmaps.
          </p>
          <center>
          <a
          href="http://www.cs.utexas.edu/users/ml/papers/venugopalan.iccv15.pdf" class="btn btn-danger" role="button">PDF</a>
          <a
          href="https://www.cs.utexas.edu/~vsub/pdf/S2VT_slides.pdf" class="btn btn-success"
          role="button">Slides</a>
          <a
          href="https://www.cs.utexas.edu/~vsub/pdf/S2VT_poster.pdf"
          class="btn btn-warning"
          role="button">Poster</a>
          </center>
          </div>

    <hr class="soften"></hr>

          <div class="project-page">
          <a name="overview"></a>
          <h2>Overview</h2>
          <center>
          <div class="approachimg">
            <img class="center-block" src="imgs/lm_fusion/overview.png" alt="overview"></img>
          </div>
          <p class="lead">An overview of the approach.</p>
          </center>
          <center>
          <div class="embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item"
          src="https://www.youtube.com/embed/-xNI7e7YgDk"
          allowfullscreen></iframe>
          </div>
          <p class="lead">
          ICCV 2015 Spotlight Video.
          </p>
          </center>


          </div>

    <hr class="soften"></hr>

          <div class="project-page">
          <a name="examples"></a>
          <h2>Examples</h2>
          <center>
          <div class="col-md-6">
            <div class="embed-responsive embed-responsive-16by9">
            <iframe class="embed-responsive-item"
            src="https://www.youtube.com/embed/XTq0huTXj1M"
            allowfullscreen></iframe>
            </div>
            <p class="lead">
            Sample clips from MPII-MD dataset.
            </p>
          </div>
          <div class="col-md-6">
            <div class="embed-responsive embed-responsive-16by9">
            <iframe class="embed-responsive-item"
            src="https://www.youtube.com/embed/pER0mjzSYaM"
            allowfullscreen></iframe>
            </div>
            <p class="lead">
            Sample clips from M-VAD dataset.
            </p>
          </div>
          </center>
          </div>

    <hr class="soften"></hr>

          <div class="project-page">
          <a name="code"></a>
          <h2>Code</h2>
          <p class="lead">
            The code to prepare data and train the model can be found in:
            </br>
            <a href="https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt">https://github.com/vsubhashini/caffe/tree/recurrent/examples/s2vt</a>
            </br>
            </br>
            Model information:<a
            href="https://gist.github.com/vsubhashini/38d087e140854fee4b14"> GitHub_Gist</a>
            </br>
            Download pre-trained model: <a
            href="https://www.dropbox.com/s/wn6k2oqurxzt6e2/s2s_vgg_pstream_allvocab_fac2_iter_16000.caffemodel?dl=1">S2VT_VGG_RGB_MODEL</a>
            (333MB)
            </br>
            Vocabulary:
            <a
            href="https://www.dropbox.com/s/v1lrc6leknzgn3x/yt_coco_mvad_mpiimd_vocabulary.txt?dl=0">S2VT_vocabulary</a>
            </br>
            Evaluation Code:
            <a
            href="https://github.com/vsubhashini/caption-eval">https://github.com/vsubhashini/caption-eval</a>

          </p>
            <h3>Notes:</h3>
          <p class="lead">
          <dl style="font-size:16px;">
            <dt>Caffe Compatibility</dt> <dd>
            The network is currently supported by the <code>recurrent</code> branch of the
            Caffe fork
             in <a href="https://github.com/vsubhashini/caffe.git">my
             repository</a> or <a
             href="https://github.com/jdonahue/caffe.git">Jeff's
             repository</a>
            but are not yet
            compatible with the <code>master</code> branch of
            <a href="https://github.com/BVLC/caffe/">Caffe</a>.
            </dd>
          </dl>
          </p>
          <h3>Datasets</h3>
          <p class="lead">
            The datasets used in the paper are available at these links:
            </br>
          <dl style="font-size:16px;">
            Microsoft Video Description Dataset (Youtube videos):
            </br>
            <a
            href="http://www.cs.utexas.edu/users/ml/clamp/videoDescription/">Project
            Page - http://www.cs.utexas.edu/users/ml/clamp/videoDescription/</a>
            </br><a
            href="http://research.microsoft.com/en-us/downloads/38cf15fd-b8df-477e-a4e4-a4680caa75af/default.aspx">[Raw Data Download
            Link]</a>
            <a
            href="https://www.dropbox.com/sh/4ecwl7zdha60xqo/AAC_TAsR7SkEYhkSdAFKcBlMa?dl=0">[PROCESSED_DATA]</a>
            </br>
            MPII Movie Description (MPII-MD) Dataset:
            </br>
            <a
            href="http://www.mpi-inf.mpg.de/movie-description">http://www.mpi-inf.mpg.de/movie-description</a>
            </br>
            Montreal Video Annotation Description (M-VAD) Dataset:
            </br>
            <a
            href="http://www.mila.umontreal.ca/Home/public-datasets/montreal-video-annotation-dataset">http://www.mila.umontreal.ca/Home/public-datasets/montreal-video-annotation-dataset</a>
            </br>
            </dl>
          </p>
          </div>

    <hr class="soften"></hr>

          <div class="project-page">
          <a name="refs"></a>
          <h2>Reference</h2>
          <p class="lead">
          If you find this useful in your work please consider citing:
          <div class="highlight">
          <pre> <code> 
          @inproceedings{venugopalan15iccv,
          title = {Sequence to Sequence -- Video to Text},
          author = {Venugopalan, Subhashini and Rohrbach, Marcus and Donahue, Jeff 
                    and Mooney, Raymond and Darrell, Trevor and Saenko, Kate},
          booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
          year = {2015}
          }
 </code> </pre>

          </div>

    </div>
    </div> <!--container-->

    <footer class="footer">
        <div class="container">
            <p class="text-muted text-center" style="padding-top: 10px">
                <a title="Creative Commons Attribution 4.0 International license" target="_blank" href="http://creativecommons.org/licenses/by/4.0/" rel="license">
                  <img alt="License" src="imgs/cca4-88x31.png"></img>
                </a>
            </p>
        </div>
    </footer>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="js/bootstrap.min.js"></script>
    <script src="js/toggle.js"></script>
  </body>
</html>

